{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Reproducible Analysis Pipeline for Journal of Phonetics\n",
        "\n",
        "**Description:** This notebook contains the complete data processing and statistical analysis pipeline for the study. It includes raw data extraction, filtering, and Generalized Additive Mixed Modeling (GAMM) using `mgcv`.\n",
        "**System Requirements:** Google Colab (Linux environment), Python 3.x, R 4.x.\n",
        "\n",
        "---\n",
        "\n",
        "### Cell 1: Setup & Global Configuration"
      ],
      "metadata": {
        "id": "mWovhxHAK6wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: Environment Setup & Configuration\n",
        "# ==============================================================================\n",
        "# [User Action Required] Set your main project path here.\n",
        "# All inputs/outputs will be relative to this path.\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Rate-F0_Cova\"\n",
        "\n",
        "# --- 1. System Setup ---\n",
        "!pip install -q praat-parselmouth pandas tqdm joblib\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. Global Configuration ---\n",
        "CONFIG = {\n",
        "    # Input Paths (Raw Data)\n",
        "    'input_buckeye': os.path.join(BASE_DRIVE_PATH, \"data/01_raw/buckeye/\"),\n",
        "    'input_csj_m':   os.path.join(BASE_DRIVE_PATH, \"data/01_raw/csj/\"),   # Monologue\n",
        "    'input_csj_d':   os.path.join(BASE_DRIVE_PATH, \"data/01_raw/csj_d/\"), # Dialogue\n",
        "\n",
        "    # Output Paths\n",
        "    'output_file':   os.path.join(BASE_DRIVE_PATH, \"data/02_interim/rate_f0_master.csv\"),\n",
        "    'supp_dir':      os.path.join(BASE_DRIVE_PATH, \"results/supplement/\"),\n",
        "\n",
        "    # Local Cache (for faster processing on Colab)\n",
        "    'local_root':    \"/content/data\",\n",
        "\n",
        "    # Analysis Parameters\n",
        "    'pitch_range': (75, 600),\n",
        "    'vowels_jp': {'a', 'i', 'u', 'e', 'o'},\n",
        "    'vowels_en': {'aa', 'ae', 'ah', 'ao', 'aw', 'ay', 'eh', 'er', 'ey', 'ih', 'iy', 'ow', 'oy', 'uh', 'uw'},\n",
        "\n",
        "    # Filtering Thresholds (Strict)\n",
        "    'dur_range_strict': (0.03, 0.50), # 30ms - 500ms\n",
        "    'min_valid_frames': 3,\n",
        "\n",
        "    # Pause/Boundary Labels for Exclusion\n",
        "    'pause_labels': {\n",
        "        'sp', 'pau', 'sil', 'noise', 'garbage',\n",
        "        '<sil>', '<pz>', '<cl>', '(p', '<noise>', '<vocnoise>', '{b_trans}', '{e_trans}'\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- 3. Initialize Directories & Cache ---\n",
        "for d in [os.path.dirname(CONFIG['output_file']), CONFIG['supp_dir']]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def sync_data_to_local():\n",
        "    \"\"\"Copy data from Drive to Colab local VM for speed.\"\"\"\n",
        "    print(\"üöÄ Synchronizing data to local VM (this may take a minute)...\")\n",
        "    if not os.path.exists(CONFIG['local_root']):\n",
        "        os.makedirs(CONFIG['local_root'])\n",
        "\n",
        "    # Helper to copy specific extensions\n",
        "    def copy_files(src, dst_name, exts):\n",
        "        dst = os.path.join(CONFIG['local_root'], dst_name)\n",
        "        if os.path.exists(src):\n",
        "            if not os.path.exists(dst): os.makedirs(dst)\n",
        "            for ext in exts:\n",
        "                # Quiet copy using system cp\n",
        "                !find \"{src}\" -name \"{ext}\" -exec cp -n {{}} \"{dst}/\" \\; 2>/dev/null\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Source path not found: {src}\")\n",
        "\n",
        "    copy_files(CONFIG['input_buckeye'], \"buckeye\", [\"*.wav\", \"*.phones\"])\n",
        "    copy_files(CONFIG['input_csj_m'],   \"csj\",     [\"*.wav\", \"*.TextGrid\"])\n",
        "    copy_files(CONFIG['input_csj_d'],   \"csj_d\",   [\"*.wav\", \"*.TextGrid\"])\n",
        "    print(\"‚úÖ Data synchronization complete.\")\n",
        "\n",
        "sync_data_to_local()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xT2xifv7K6wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Cell 2: Feature Extraction Classes"
      ],
      "metadata": {
        "id": "xGOzE-pQK6wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Robust Feature Extractors\n",
        "# ==============================================================================\n",
        "class BaseExtractor:\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "\n",
        "    def analyze_pitch(self, pitch, start, end):\n",
        "        \"\"\"Extract F0 max and validate voicing.\"\"\"\n",
        "        try:\n",
        "            if end <= start: return 0, 0, False\n",
        "            t1 = call(pitch, \"Get frame number from time\", start)\n",
        "            t2 = call(pitch, \"Get frame number from time\", end)\n",
        "            values = [call(pitch, \"Get value in frame\", i, \"Hertz\") for i in range(int(t1), int(t2) + 1)]\n",
        "            values = [v for v in values if v > 0] # Filter unvoiced\n",
        "\n",
        "            if not values: return 0, 0, False\n",
        "\n",
        "            f0_max = max(values)\n",
        "            has_jump = False\n",
        "            if len(values) >= 2:\n",
        "                # Check for octave jumps/errors (heuristic)\n",
        "                for k in range(len(values)-1):\n",
        "                    r = values[k] / values[k+1]\n",
        "                    if r > 1.8 or r < 0.55:\n",
        "                        has_jump = True; break\n",
        "            return f0_max, len(values), has_jump\n",
        "        except:\n",
        "            return 0, 0, False\n",
        "\n",
        "    def is_pause(self, label):\n",
        "        return str(label).lower() in CONFIG['pause_labels']\n",
        "\n",
        "class BuckeyeExtractor(BaseExtractor):\n",
        "    def run(self):\n",
        "        wav_files = sorted(glob.glob(os.path.join(self.root_dir, \"**/*.wav\"), recursive=True))\n",
        "        data = []\n",
        "        print(f\"üá∫üá∏ Processing Buckeye ({len(wav_files)} files)...\")\n",
        "\n",
        "        for wav in tqdm(wav_files):\n",
        "            try:\n",
        "                base = os.path.splitext(wav)[0]\n",
        "                # Fallback label search\n",
        "                label_path = base + \".phones\"\n",
        "                if not os.path.exists(label_path):\n",
        "                    cands = glob.glob(base + \"*.phon*\")\n",
        "                    if cands: label_path = cands[0]\n",
        "                    else: continue\n",
        "\n",
        "                sound = parselmouth.Sound(wav)\n",
        "                pitch = sound.to_pitch(time_step=0.01, pitch_floor=75, pitch_ceiling=600)\n",
        "\n",
        "                # Parse labels\n",
        "                segments = []\n",
        "                with open(label_path, 'r', errors='ignore') as f:\n",
        "                    prev_t = 0.0\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) < 3: continue\n",
        "                        try:\n",
        "                            end_t = float(parts[0])\n",
        "                            lbl = parts[2].lower()\n",
        "                            segments.append({'s': prev_t, 'e': end_t, 'l': lbl})\n",
        "                            prev_t = end_t\n",
        "                        except ValueError: continue\n",
        "\n",
        "                # Extract vowels\n",
        "                for i, seg in enumerate(segments):\n",
        "                    if seg['l'] in CONFIG['vowels_en']:\n",
        "                        f0, n_val, jump = self.analyze_pitch(pitch, seg['s'], seg['e'])\n",
        "\n",
        "                        # Context\n",
        "                        prev_l = segments[i-1]['l'] if i > 0 else \"START\"\n",
        "                        next_l = segments[i+1]['l'] if i < len(segments)-1 else \"END\"\n",
        "\n",
        "                        data.append({\n",
        "                            'Dataset': 'Buckeye', 'Language': 'English',\n",
        "                            'FileID': os.path.basename(wav), 'Speaker': os.path.basename(wav)[:3],\n",
        "                            'Vowel': seg['l'], 'Duration': seg['e'] - seg['s'],\n",
        "                            'F0_Max': f0, 'Num_Valid': n_val, 'Has_Jump': jump,\n",
        "                            'PrevSeg': prev_l, 'NextSeg': next_l,\n",
        "                            'PrevIsPause': self.is_pause(prev_l), 'NextIsPause': self.is_pause(next_l)\n",
        "                        })\n",
        "            except Exception: continue\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "class CSJExtractor(BaseExtractor):\n",
        "    def _get_target_tier_index(self, tg):\n",
        "        \"\"\"\n",
        "        Robustly identify the Phoneme tier in CSJ TextGrids.\n",
        "        Strategy: Use the 2nd Interval Tier (standard CSJ format).\n",
        "        Returns: 1-based index or -1 if failed.\n",
        "        \"\"\"\n",
        "        num_tiers = call(tg, \"Get number of tiers\")\n",
        "        interval_tiers = []\n",
        "        for i in range(1, num_tiers + 1):\n",
        "            if call(tg, \"Is interval tier\", i):\n",
        "                interval_tiers.append(i)\n",
        "\n",
        "        # CSJ standard: Tier 1 = Word/Phrase, Tier 2 = Phoneme (Seg)\n",
        "        if len(interval_tiers) >= 2:\n",
        "            return interval_tiers[1]\n",
        "        return -1\n",
        "\n",
        "    def _clean_label(self, lbl):\n",
        "        if not lbl: return \"\"\n",
        "        if lbl.startswith('<'): return lbl.lower()\n",
        "        return lbl.split('+')[0].split('_')[0].lower()\n",
        "\n",
        "    def run_generic(self, dataset_name, is_dialogue=False):\n",
        "        path_key = 'input_csj_d' if is_dialogue else 'input_csj_m'\n",
        "        # Local cache path\n",
        "        local_dir = os.path.join(CONFIG['local_root'], \"csj_d\" if is_dialogue else \"csj\")\n",
        "\n",
        "        pattern = \"**/*-[LR].wav\" if is_dialogue else \"**/*.wav\"\n",
        "        wav_files = sorted(glob.glob(os.path.join(local_dir, pattern), recursive=True))\n",
        "        # Filter mono files for monologue\n",
        "        if not is_dialogue:\n",
        "            wav_files = [w for w in wav_files if \"-L.wav\" not in w and \"-R.wav\" not in w]\n",
        "\n",
        "        print(f\"üáØüáµ Processing {dataset_name} ({len(wav_files)} files)...\")\n",
        "        data = []\n",
        "\n",
        "        for wav in tqdm(wav_files):\n",
        "            try:\n",
        "                base = os.path.basename(wav)\n",
        "                session_id = base.split('.')[0]\n",
        "                if is_dialogue: session_id = session_id[:-2] # remove -L/-R\n",
        "\n",
        "                tg_path = os.path.join(os.path.dirname(wav), session_id + \".TextGrid\")\n",
        "                if not os.path.exists(tg_path): continue\n",
        "\n",
        "                tg = parselmouth.read(tg_path)\n",
        "                sound = parselmouth.Sound(wav)\n",
        "                pitch = sound.to_pitch(time_step=0.01, pitch_floor=75, pitch_ceiling=600)\n",
        "\n",
        "                tgt_tier = self._get_target_tier_index(tg)\n",
        "                if tgt_tier == -1: continue\n",
        "\n",
        "                n_ints = call(tg, \"Get number of intervals\", tgt_tier)\n",
        "                for i in range(1, n_ints+1):\n",
        "                    lbl_raw = call(tg, \"Get label of interval\", tgt_tier, i)\n",
        "                    lbl = self._clean_label(lbl_raw)\n",
        "\n",
        "                    if lbl in CONFIG['vowels_jp']:\n",
        "                        s = call(tg, \"Get start time of interval\", tgt_tier, i)\n",
        "                        e = call(tg, \"Get end time of interval\", tgt_tier, i)\n",
        "                        f0, n_val, jump = self.analyze_pitch(pitch, s, e)\n",
        "\n",
        "                        # Context\n",
        "                        prev_raw = call(tg, \"Get label of interval\", tgt_tier, i-1) if i>1 else \"START\"\n",
        "                        next_raw = call(tg, \"Get label of interval\", tgt_tier, i+1) if i<n_ints else \"END\"\n",
        "\n",
        "                        row = {\n",
        "                            'Dataset': dataset_name, 'Language': 'Japanese',\n",
        "                            'FileID': base, 'Vowel': lbl, 'Duration': e - s,\n",
        "                            'F0_Max': f0, 'Num_Valid': n_val, 'Has_Jump': jump,\n",
        "                            'PrevSeg': self._clean_label(prev_raw), 'NextSeg': self._clean_label(next_raw),\n",
        "                            'PrevIsPause': self.is_pause(prev_raw), 'NextIsPause': self.is_pause(next_raw)\n",
        "                        }\n",
        "\n",
        "                        # Identity Logic\n",
        "                        if is_dialogue:\n",
        "                            channel = \"L\" if \"-L\" in base else \"R\"\n",
        "                            row['Session'] = session_id\n",
        "                            row['Speaker'] = f\"{session_id}-{channel}\" # Unique per channel\n",
        "                        else:\n",
        "                            row['Speaker'] = session_id # Monologue file = Speaker\n",
        "\n",
        "                        data.append(row)\n",
        "            except Exception: continue\n",
        "        return pd.DataFrame(data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "X6baVJlHK6wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Cell 3: Data Processing & Feature Engineering"
      ],
      "metadata": {
        "id": "QaFX51q1K6wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# Cell 3: Data Processing & Dataset Generation\n",
        "# ==============================================================================\n",
        "FORCE_REGENERATE = True # Set to False to reload from CSV\n",
        "\n",
        "if os.path.exists(CONFIG['output_file']) and not FORCE_REGENERATE:\n",
        "    print(f\"üìÇ Loading existing dataset: {CONFIG['output_file']}\")\n",
        "    df_full = pd.read_csv(CONFIG['output_file'])\n",
        "else:\n",
        "    # 1. Run Extractors\n",
        "    df_buck = BuckeyeExtractor(os.path.join(CONFIG['local_root'], \"buckeye\")).run()\n",
        "    df_mono = CSJExtractor(CONFIG['local_root']).run_generic(\"CSJ_Monologue\", is_dialogue=False)\n",
        "    df_dial = CSJExtractor(CONFIG['local_root']).run_generic(\"CSJ_Dialogue\", is_dialogue=True)\n",
        "\n",
        "    df_raw = pd.concat([df_buck, df_mono, df_dial], ignore_index=True)\n",
        "    print(f\"üìä Total Raw Tokens: {len(df_raw):,}\")\n",
        "\n",
        "    # 2. Sanity Check\n",
        "    csj_mono_count = len(df_raw[df_raw['Dataset'] == 'CSJ_Monologue'])\n",
        "    if csj_mono_count < 10000:\n",
        "        print(f\"‚ö†Ô∏è WARNING: CSJ Monologue count is suspiciously low ({csj_mono_count}). Check Tier logic.\")\n",
        "\n",
        "    # 3. Filtering Flags\n",
        "    df_raw['Flag_Dur'] = ~df_raw['Duration'].between(CONFIG['dur_range_strict'][0], CONFIG['dur_range_strict'][1])\n",
        "    df_raw['Flag_Unvoiced'] = df_raw['Num_Valid'] == 0\n",
        "    df_raw['Flag_Sparse'] = (df_raw['Num_Valid'] > 0) & (df_raw['Num_Valid'] < CONFIG['min_valid_frames'])\n",
        "    df_raw['Flag_Jump'] = df_raw['Has_Jump']\n",
        "    df_raw['Exclude'] = df_raw[['Flag_Dur', 'Flag_Unvoiced', 'Flag_Sparse', 'Flag_Jump']].any(axis=1)\n",
        "\n",
        "    # 4. Save Filtering Statistics\n",
        "    # (A) Exclusive Filtering\n",
        "    def calc_exclusive(df, name):\n",
        "        total = len(df)\n",
        "        step1 = df[df['Flag_Dur']]\n",
        "        rem1 = df[~df['Flag_Dur']]\n",
        "        step2 = rem1[rem1['Flag_Unvoiced']]\n",
        "        rem2 = rem1[~rem1['Flag_Unvoiced']]\n",
        "        step3 = rem2[rem2['Flag_Sparse']]\n",
        "        rem3 = rem2[~rem2['Flag_Sparse']]\n",
        "        step4 = rem3[rem3['Flag_Jump']]\n",
        "        kept = rem3[~rem3['Flag_Jump']]\n",
        "        return {'Dataset': name, 'Total': total, 'Drop_Duration': len(step1), 'Drop_Unvoiced': len(step2),\n",
        "                'Drop_Sparse': len(step3), 'Drop_Jump': len(step4), 'Final_Kept': len(kept)}\n",
        "\n",
        "    excl_stats = [calc_exclusive(df_raw[df_raw['Dataset']==ds], ds) for ds in df_raw['Dataset'].unique()]\n",
        "    pd.DataFrame(excl_stats).to_csv(os.path.join(CONFIG['supp_dir'], \"TableS_filtering_summary_exclusive.csv\"), index=False)\n",
        "\n",
        "    # 5. Clean & Engineer Features\n",
        "    df_full = df_raw[~df_raw['Exclude']].copy()\n",
        "\n",
        "    # Semitone Conversion (ref 1Hz)\n",
        "    df_full['F0_ST'] = 12 * np.log2(df_full['F0_Max'] / 1)\n",
        "\n",
        "    # Winsorization (per speaker)\n",
        "    def winsor(g): return g.clip(upper=g.quantile(0.995))\n",
        "    df_full['F0_Max_Winsor'] = df_full.groupby('Speaker')['F0_Max'].transform(winsor)\n",
        "    df_full['F0_ST_Winsor'] = 12 * np.log2(df_full['F0_Max_Winsor'] / 1)\n",
        "\n",
        "    # Mundlak Decomposition (Within/Between)\n",
        "    spk_mean = df_full.groupby('Speaker')['Duration'].transform('mean')\n",
        "    df_full['Duration_Between'] = spk_mean\n",
        "    df_full['Duration_Within'] = df_full['Duration'] - spk_mean\n",
        "\n",
        "    # Fill NAs for R compatibility\n",
        "    for c in ['Speaker', 'Language', 'Dataset', 'Session', 'NextSeg']:\n",
        "        if c in df_full.columns: df_full[c] = df_full[c].fillna('NA').astype(str)\n",
        "\n",
        "    # Save\n",
        "    df_full.to_csv(CONFIG['output_file'], index=False)\n",
        "    print(f\"‚úÖ Master dataset saved: {len(df_full):,} rows.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "CcnJf7zCK6wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Cell 4: R Environment & Functions"
      ],
      "metadata": {
        "id": "20vJbyVNK6wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext rpy2.ipython\n",
        "\n",
        "%%R -i df_full\n",
        "# ==============================================================================\n",
        "# Cell 4: R Statistical Analysis Setup (mgcv)\n",
        "# ==============================================================================\n",
        "library(mgcv)\n",
        "library(dplyr)\n",
        "\n",
        "# --- Configuration ---\n",
        "SUPP_DIR <- \"/content/drive/MyDrive/Rate-F0_Cova/results/supplement/\"\n",
        "AUDIT_FILE <- file.path(SUPP_DIR, \"TableS_N_audit.csv\")\n",
        "dir.create(SUPP_DIR, showWarnings = FALSE, recursive = TRUE)\n",
        "\n",
        "# --- Helper 1: N-Audit System ---\n",
        "# Records strict N counts for every model to ensure reproducibility\n",
        "record_audit <- function(model_obj, model_id, study_label, input_df) {\n",
        "  n_input <- nrow(input_df)\n",
        "  n_used <- nobs(model_obj)\n",
        "  n_spk <- length(unique(input_df$Speaker))\n",
        "\n",
        "  # Append to CSV\n",
        "  row <- data.frame(\n",
        "    timestamp = Sys.time(), model_id = model_id, study = study_label,\n",
        "    n_input = n_input, n_used = n_used, n_dropped = n_input - n_used,\n",
        "    n_speakers = n_spk\n",
        "  )\n",
        "  write.table(row, AUDIT_FILE, sep=\",\", append=file.exists(AUDIT_FILE),\n",
        "              col.names=!file.exists(AUDIT_FILE), row.names=FALSE)\n",
        "  cat(sprintf(\"[Audit] %s (Study %s): N=%d (Used=%d)\\n\", model_id, study_label, n_input, n_used))\n",
        "}\n",
        "\n",
        "# --- Helper 2: Strict Fit Wrapper ---\n",
        "# Forces correct method/discrete settings and runs audit\n",
        "fit_bam_strict <- function(formula, data, mod_id, study_lbl, method=\"fREML\", discrete=TRUE) {\n",
        "  cat(paste(\"\\nüöÄ Fitting:\", mod_id, \"...\\n\"))\n",
        "  m <- bam(formula, data=data, method=method, discrete=discrete, nthreads=2)\n",
        "  record_audit(m, mod_id, study_lbl, data)\n",
        "  return(m)\n",
        "}\n",
        "\n",
        "# --- Data Preparation ---\n",
        "cat(\"üõ†Ô∏è Preparing Analysis Subsets...\\n\")\n",
        "data_all <- df_full\n",
        "# Factor conversion\n",
        "cols_fac <- c(\"Speaker\", \"Vowel\", \"Language\", \"Dataset\", \"Session\")\n",
        "data_all[cols_fac] <- lapply(data_all[cols_fac], as.factor)\n",
        "\n",
        "# Define Main Subsets\n",
        "study1_tok <- subset(data_all, Dataset %in% c(\"CSJ_Monologue\", \"Buckeye\")) %>% droplevels()\n",
        "study2_tok <- subset(data_all, Dataset == \"CSJ_Dialogue\") %>% droplevels()\n",
        "\n",
        "# Create Fixed Datasets (Complete Cases for ML comparison)\n",
        "s1_cc <- complete.cases(study1_tok[, c(\"F0_ST\", \"Duration\", \"Speaker\", \"Vowel\", \"Language\")])\n",
        "study1_ml_dat <- study1_tok[s1_cc, ]\n",
        "\n",
        "s2_cc <- complete.cases(study2_tok[, c(\"F0_ST\", \"Duration\", \"Speaker\", \"Vowel\", \"Session\")])\n",
        "study2_ml_dat <- study2_tok[s2_cc, ]\n",
        "\n",
        "cat(sprintf(\"‚úÖ Data Ready: Study 1 (N=%d), Study 2 (N=%d)\\n\", nrow(study1_ml_dat), nrow(study2_ml_dat)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vHibQCMsK6wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Cell 5: Main Analysis Execution"
      ],
      "metadata": {
        "id": "7utv4a8WK6wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# ==============================================================================\n",
        "# Cell 5: Main Statistical Analysis Execution\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Task A: ML Model Comparison (AIC) ---\n",
        "cat(\"\\n=== Task A: ML Model Comparison ===\\n\")\n",
        "\n",
        "# Study 1 Formulas\n",
        "f1_full <- F0_ST ~ Language + s(Duration, by=Language, k=20) + s(Duration, Speaker, bs=\"fs\", m=1, k=5) + s(Vowel, bs=\"re\")\n",
        "f1_base <- F0_ST ~ Language + s(Duration, by=Language, k=20) + s(Speaker, bs=\"re\") + s(Vowel, bs=\"re\")\n",
        "\n",
        "m1_full <- fit_bam_strict(f1_full, study1_ml_dat, \"s1_full_ML\", \"1\", method=\"ML\", discrete=FALSE)\n",
        "m1_base <- fit_bam_strict(f1_base, study1_ml_dat, \"s1_base_ML\", \"1\", method=\"ML\", discrete=FALSE)\n",
        "write.csv(AIC(m1_full, m1_base), file.path(SUPP_DIR, \"TableS_ML_comparison_study1.csv\"))\n",
        "\n",
        "# Study 2 Formulas\n",
        "f2_full <- F0_ST ~ s(Duration, k=20) + s(Duration, Speaker, bs=\"fs\", m=1, k=5) + s(Vowel, bs=\"re\")\n",
        "f2_base <- F0_ST ~ s(Duration, k=20) + s(Speaker, bs=\"re\") + s(Vowel, bs=\"re\")\n",
        "\n",
        "m2_full <- fit_bam_strict(f2_full, study2_ml_dat, \"s2_full_ML\", \"2\", method=\"ML\", discrete=FALSE)\n",
        "m2_base <- fit_bam_strict(f2_base, study2_ml_dat, \"s2_base_ML\", \"2\", method=\"ML\", discrete=FALSE)\n",
        "write.csv(AIC(m2_full, m2_base), file.path(SUPP_DIR, \"TableS_ML_comparison_study2.csv\"))\n",
        "\n",
        "\n",
        "# --- Task B: k-check Diagnostics ---\n",
        "cat(\"\\n=== Task B: k-check Diagnostics ===\\n\")\n",
        "sink(file.path(SUPP_DIR, \"TableS_kcheck_log.txt\"))\n",
        "set.seed(123)\n",
        "gam.check(m1_full)\n",
        "gam.check(m2_full)\n",
        "sink()\n",
        "\n",
        "\n",
        "# --- Task C: Session Variance Components (Robust) ---\n",
        "cat(\"\\n=== Task C: Session Variance Components ===\\n\")\n",
        "# Formula with Session Random Effect\n",
        "f_re <- F0_ST ~ s(Duration, k=20) + s(Duration, Speaker, bs=\"fs\", m=1, k=5) + s(Session, bs=\"re\") + s(Vowel, bs=\"re\")\n",
        "m_re <- fit_bam_strict(f_re, study2_ml_dat, \"s2_session_re\", \"2\", method=\"fREML\", discrete=TRUE)\n",
        "\n",
        "# Robust Extraction\n",
        "vc_raw <- gam.vcomp(m_re)\n",
        "if(is.list(vc_raw)) { vc_vec <- unlist(vc_raw); vc_mat <- matrix(vc_vec, ncol=1); rownames(vc_mat) <- names(vc_vec) } else { vc_mat <- as.matrix(vc_raw) }\n",
        "\n",
        "# Build Summary Table\n",
        "terms <- rownames(vc_mat)\n",
        "std_devs <- as.numeric(vc_mat[, 1])\n",
        "comp_type <- rep(\"Other\", length(terms))\n",
        "comp_type[grepl(\"Session\", terms)] <- \"Session\"\n",
        "comp_type[grepl(\"scale\", terms)] <- \"Residual\"\n",
        "\n",
        "df_vc <- data.frame(term=terms, std_dev=std_devs, component=comp_type)\n",
        "write.csv(df_vc, file.path(SUPP_DIR, \"TableS_session_vcomp.csv\"), row.names=FALSE)\n",
        "\n",
        "\n",
        "# --- Task D: Buckeye Segmental Control (Pre-fortis clipping) ---\n",
        "cat(\"\\n=== Task D: Buckeye Segmental Control ===\\n\")\n",
        "# Extract Buckeye & Create Control Variable\n",
        "buck_dat <- subset(study1_tok, Dataset==\"Buckeye\")\n",
        "buck_dat$NextVoiceless <- ifelse(trimws(as.character(buck_dat$NextSeg)) %in% c(\"p\",\"t\",\"k\",\"f\",\"th\",\"s\",\"sh\",\"ch\",\"hh\"), \"Voiceless\", \"Voiced\")\n",
        "buck_dat$NextVoiceless <- as.factor(buck_dat$NextVoiceless)\n",
        "buck_cc <- buck_dat[complete.cases(buck_dat[, c(\"F0_ST\", \"Duration\", \"NextVoiceless\")]), ]\n",
        "\n",
        "# Compare Models\n",
        "f_bk_base <- F0_ST ~ s(Duration, k=20) + s(Duration, Speaker, bs=\"fs\", m=1, k=5) + s(Vowel, bs=\"re\")\n",
        "f_bk_ctrl <- F0_ST ~ NextVoiceless + s(Duration, k=20) + s(Duration, Speaker, bs=\"fs\", m=1, k=5) + s(Vowel, bs=\"re\")\n",
        "\n",
        "m_bk_base <- fit_bam_strict(f_bk_base, buck_cc, \"buck_base\", \"1_bk\", method=\"ML\", discrete=FALSE)\n",
        "m_bk_ctrl <- fit_bam_strict(f_bk_ctrl, buck_cc, \"buck_ctrl\", \"1_bk\", method=\"ML\", discrete=FALSE)\n",
        "\n",
        "res_bk <- data.frame(\n",
        "    AIC_Base = AIC(m_bk_base), AIC_Ctrl = AIC(m_bk_ctrl),\n",
        "    Delta_AIC = AIC(m_bk_ctrl) - AIC(m_bk_base),\n",
        "    Coef_Voiceless = summary(m_bk_ctrl)$p.table[grep(\"Voiceless\", rownames(summary(m_bk_ctrl)$p.table)), \"Estimate\"]\n",
        ")\n",
        "write.csv(res_bk, file.path(SUPP_DIR, \"TableS_buckeye_control.csv\"), row.names=FALSE)\n",
        "\n",
        "cat(\"\\n‚úÖ All analyses completed successfully. Check 'results/supplement/' folder.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wG_EIfPSK6wi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}